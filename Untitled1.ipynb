{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract packages\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together everything from before\n",
    "\n",
    "def scrape_events(events_page_url):\n",
    "    #Your code here\n",
    "    page = requests.get(events_page_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # set up the container\n",
    "    container = soup.find(id = 'items')\n",
    "    \n",
    "    # set up each row of the container, we will be iterating over each row of the container\n",
    "    rows = container.findAll('li') # each row is identifiable by its tag 'li'\n",
    "    \n",
    "    # traverse the container; searching by h1 heading should narrow things down\n",
    "    eventdata = container.findAll('h1', class_ ='event-title') # eventdata contains both name and venue\n",
    "    \n",
    "    # write a loop that iterates through the container for a list of names that will be used in the dataframe (final_name_list)\n",
    "    final_name_list = [ ]\n",
    "\n",
    "    #set up a list that contains the list of names\n",
    "    #this list will be used by the for loop to populate the final_name_list \n",
    "    #ONLY where an event-title exists.\n",
    "    name_list = [ ]\n",
    "\n",
    "    # this loop is used to populate a list of event names for use in name_list\n",
    "    for n in range(len(eventdata)):\n",
    "        name = eventdata[n].find('a').text\n",
    "        name_list.append(name)\n",
    "\n",
    "    # this loop is used to check if there is an event name in a row, if not, return NaN\n",
    "    # if there is an event name, it takes the actual name of the event from name_list and uses it to populate final_name_list\n",
    "    # final_name_list goes into final dataframe\n",
    "\n",
    "    name_list_counter = 0\n",
    "\n",
    "    for n in range(len(rows)):\n",
    "    \n",
    "        if rows[n].find(class_ = 'event-title') == None:\n",
    "            final_name_list.append('NaN')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            final_name_list.append(name_list[name_list_counter])\n",
    "            name_list_counter += 1\n",
    "   \n",
    "    # set up a list of venues actually found in the container that will be used to populate final_venue_list\n",
    "    venue_list = [ ]\n",
    "\n",
    "    # set up an empty list of final_venue_list which will be used to construct the dataframe\n",
    "    final_venue_list = [ ]\n",
    "\n",
    "    # this loop is used to populate the event names actually found in the container that will, in turn, be used to\n",
    "    # populate the final_venue_list\n",
    "    for v in range(len(eventdata)):\n",
    "        venue = eventdata[v].find('span').text.strip('at ')\n",
    "        venue_list.append(venue)\n",
    "\n",
    "    # this loop is used to populate the final_venue_list used in the dataframe, it will search each row of the container\n",
    "    # where it does not find a venue name in a row, it will return NaN and populate final_venue_list accordingly\n",
    "    # where it does find a venue name, it will take a name from the venue_list and populate final_venue_list accordingly.\n",
    "\n",
    "    venue_list_counter = 0\n",
    "\n",
    "    for v in range(len(rows)):\n",
    "    \n",
    "        if rows[v].find(class_ = 'event-title') == None:\n",
    "            final_venue_list.append('NaN')\n",
    "        else:\n",
    "            final_venue_list.append(venue_list[venue_list_counter])\n",
    "            venue_list_counter += 1 \n",
    "    \n",
    "    # write a loop that iterates through the container for the date of the event to generate a list\n",
    "    # NOTE: certain dates have more events than others\n",
    "    final_date_list = [ ]\n",
    "\n",
    "    for d in range(len(rows)):\n",
    "        if rows[d].find(class_ = 'eventDate date') == None:\n",
    "            final_date_list.append(date)\n",
    "        else:\n",
    "            date = rows[d].find('a').text.strip(' /')\n",
    "            final_date_list.append(date) \n",
    "    \n",
    "    #set up an empty list, populating with number of attendees specified by the page.\n",
    "    attend_list = [ ]\n",
    "\n",
    "    #set up a final_attend_list to be used to construct the dataframe, this is populated with\n",
    "    #the figures from attend_list where available and populated with NaN if not available.\n",
    "    final_attend_list = [ ]\n",
    "    \n",
    "    # This narrows down into listings that do have attendees\n",
    "    attend = container.findAll('p', class_ = 'attending')\n",
    "\n",
    "    # this loop populates attend_list and creates a list of number of attendees where they have been specified\n",
    "    for a in range(len(attend)):\n",
    "        a_num = int(attend[a].text.strip('Attending '))\n",
    "        attend_list.append(a_num)\n",
    "\n",
    "    # this loop populates the final_attend_list such with numbers from attend_list where available, otherwise for listings\n",
    "    # that do not specify any number of attendees, NaN is returned.\n",
    "\n",
    "    attend_list_counter = 0\n",
    "\n",
    "    for a in range(len(rows)):\n",
    "        \n",
    "        if rows[a].find(class_ = 'attending') == None:\n",
    "            final_attend_list.append(0)\n",
    "            \n",
    "        else:\n",
    "            final_attend_list.append(attend_list[attend_list_counter])\n",
    "            attend_list_counter +=1\n",
    "    \n",
    "    final_lists = [final_name_list, final_venue_list, final_date_list, final_attend_list]\n",
    "    \n",
    "    # construct the dataframe\n",
    "    df = pd.DataFrame(final_lists).transpose()\n",
    "    df.columns = [\"Event_Name\", \"Venue\", \"Event_Date\", \"Number_of_Attendees\"]\n",
    "\n",
    "    #columns where the Event Name is NaN can be removed.\n",
    "    df = df.loc[df['Event_Name'] != 'NaN']\n",
    "\n",
    "    # this resets the index and drops the old index which doesn't work anymore now that the NaNs have been removed.\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page(url):\n",
    "    #set up the page and the soup\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # find the section in the soup where the previous and next buttons can be found\n",
    "    next_container = soup.find('div', class_ = 'page-items content sub clearfix')\n",
    "    \n",
    "    # this will list links to both the previous AND the next buttons\n",
    "    buttons = next_container.findAll('a', href=True)\n",
    "\n",
    "    # the next button is index number 1 in the buttons list, specifying ['href'] like that should get the url for the next button\n",
    "    next = buttons[1]['href']\n",
    "    \n",
    "    # Now, a bit of string manipulation to join everything up, use a literal to complete the url and ensure it is a string\n",
    "    # This is what the full url looks like: https://www.residentadvisor.net/events/uk/london/week/2020-06-22\n",
    "\n",
    "    next_page_url = str(print('https://www.residentadvisor.net{}'.format(next)))\n",
    "    \n",
    "    return next_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.residentadvisor.net/events/uk/london/week/2020-05-25\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'None': No schema supplied. Perhaps you meant http://None?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c1a0fcf5212c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mdf_length\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mnext_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdf_large\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_large\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdf_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_large\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-849630466733>\u001b[0m in \u001b[0;36mscrape_events\u001b[1;34m(events_page_url)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mscrape_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevents_page_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#Your code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevents_page_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         )\n\u001b[1;32m--> 519\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerge_setting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerged_cookies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerge_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m         )\n\u001b[0;32m    464\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'None': No schema supplied. Perhaps you meant http://None?"
     ]
    }
   ],
   "source": [
    "# construct an initial dataframe of the initial page where scraping begins\n",
    "start_url = 'https://www.residentadvisor.net/events'\n",
    "df_large = scrape_events(start_url)\n",
    "\n",
    "# get the next url; there will definitely be need of another url, we aren't going to get 1000 listings in 1 page....\n",
    "\n",
    "new_url = next_page(start_url)\n",
    "\n",
    "# scrape the next page and add the dataframe constructed thereby to the initial dataframe\n",
    "# remember to test the length of the dataframe \n",
    "# keep running the loop to scrape the following page until the length of the dataframe = 500\n",
    "\n",
    "df_length = len(df_large)\n",
    "\n",
    "while df_length < 300:\n",
    "    next_df = scrape_events(new_url)\n",
    "    df_large = pd.concat([df_large, next_df])\n",
    "    df_length = len(df_large)\n",
    "    if df_length >= 300:\n",
    "        break\n",
    "    new_url = next_page(new_url)\n",
    "\n",
    "# tidy up the index\n",
    "df_large.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
